{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5c93ab-2e5a-4d77-966b-45f71e556946",
   "metadata": {},
   "source": [
    "# 1. With Human Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "648e8a1d-0fdf-4b88-b6b3-176623aab17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ae6fa38-72be-4be6-9c19-f18499a74395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "481039c2-f814-4ed2-b888-99bff9aa41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d230657-2266-4bda-9cee-d876ebc13f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model): # Agent function (our model)\n",
    "    def __init__(self, num_actions, num_hidden_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        \n",
    "    def call(self, input_obs):\n",
    "        x = self.shared_1(input_obs)\n",
    "        return self.actor(x)\n",
    "    \n",
    "model = Actor(num_actions=2, num_hidden_units=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533a214-569e-47db-91b3-5ef949949c42",
   "metadata": {},
   "source": [
    "This code defines a class **Actor** that represents an agent in reinforcement learning using a neural network model. The class is a subclass of **tf.keras.Model** and uses the **tf.keras** library, which is a high-level API for building and training deep learning models.\n",
    "\n",
    "The class has two class variables:\n",
    "\n",
    "- **shared_1**: a dense layer with **num_hidden_units** number of neurons and the activation function **'relu'** (rectified linear unit).\n",
    "- **actor**: a dense layer with **num_actions** number of neurons, representing the actions the agent can take in the environment.\n",
    "\n",
    "The **call** method takes in an input **input_obs** (representing the state of the environment), applies the shared dense layer to the input, and returns the output from the actor dense layer.\n",
    "\n",
    "Finally, an instance of the class **Actor** is created by calling **model = Actor(num_actions=2, num_hidden_units=128)**, where **num_actions=2** and **num_hidden_units=128** are the number of actions the agent can take and the number of hidden units in the shared dense layer, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "84eaf161-fb00-4c6b-8e6c-18f9166a0833",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m human_preference_reward \u001b[38;5;241m=\u001b[39m \u001b[43mask_human_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m human_preference_reward\n",
      "Cell \u001b[0;32mIn[67], line 6\u001b[0m, in \u001b[0;36mask_human_preference\u001b[0;34m(state, action, next_state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_human_preference\u001b[39m(state, action, next_state):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Asks the human for their preference between the current action and a random alternative action.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Returns the reward based on the human's preference.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     current_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you prefer action \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maction\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m leading to \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnext_state\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m over a random action? (1 for Yes, 0 for No): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     random_action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m!=\u001b[39m action])\n\u001b[1;32m      8\u001b[0m     random_next_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(random_action)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1006\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[0;32m-> 1006\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1051\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "obs = env.reset()[0]\n",
    "action_probs_list = []\n",
    "rewards = []\n",
    "done = False\n",
    "obs = tf.expand_dims(obs, 0)\n",
    "while True:\n",
    "    if done:\n",
    "        break\n",
    "    # run model to get action logits and value\n",
    "    action_logits = model(obs)\n",
    "    # categorical probabilistic action idx selection\n",
    "    discrete_distribution = tfp.distributions.Categorical(action_logits)\n",
    "    selection_action_idx = discrete_distribution.sample()\n",
    "\n",
    "    action = int(selection_action_idx.numpy()[0])\n",
    "\n",
    "    new_obs = env.step(action)[0]\n",
    "    reward = env.step(action)[1]\n",
    "    done = env.step(action)[2]\n",
    "\n",
    "    human_preference_reward = ask_human_preference(obs.numpy(), action, new_obs)\n",
    "    print('yes')\n",
    "    reward += human_preference_reward\n",
    "\n",
    "    action_probs = discrete_distribution.prob(selection_action_idx)\n",
    "    action_probs_list.append(action_probs)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d39e562-e783-4aaa-8f7a-cfe52f94d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_human_preference(state, action, next_state):\n",
    "    \"\"\"\n",
    "    Asks the human for their preference between the current action and a random alternative action.\n",
    "    Returns the reward based on the human's preference.\n",
    "    \"\"\"\n",
    "    current_reward = int(input(f\"Do you prefer action {action} leading to {next_state} over a random action? (1 for Yes, 0 for No): \"))\n",
    "    random_action = random.choice([a for a in range(env.action_space.n) if a != action])\n",
    "    random_next_state = env.step(random_action)[0]\n",
    "    alternative_reward = int(input(f\"Do you prefer action {random_action} leading to {random_next_state} over action {action} leading to {next_state}? (1 for Yes, 0 for No): \"))\n",
    "    return current_reward - alternative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d2112fd-4c3c-413d-939f-31ff5bb7521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_episode(env, model):\n",
    "    obs = env.reset()[0]\n",
    "    action_probs_list = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    obs = tf.expand_dims(obs, 0)\n",
    "    while True:\n",
    "        if done:\n",
    "            break\n",
    "        # run model to get action logits and value\n",
    "        action_logits = model(obs)\n",
    "        # categorical probabilistic action idx selection\n",
    "        discrete_distribution = tfp.distributions.Categorical(action_logits)\n",
    "        selection_action_idx = discrete_distribution.sample()\n",
    "        \n",
    "        action = int(selection_action_idx.numpy()[0])\n",
    "        new_obs = env.step(action)[0]\n",
    "        reward = env.step(action)[1]\n",
    "        done = env.step(action)[2]\n",
    "        #print(done)\n",
    "        #human_preference_reward = ask_human_preference(obs.numpy(), action, new_obs)\n",
    "        #print('yes')\n",
    "        #reward += human_preference_reward\n",
    "        \n",
    "        action_probs = discrete_distribution.prob(selection_action_idx)\n",
    "        action_probs_list.append(action_probs)\n",
    "        rewards.append(reward)\n",
    "        obs = tf.expand_dims(new_obs, 0)\n",
    "    human_preference_reward = ask_human_preference(obs.numpy(), action, new_obs)\n",
    "    rewards[-1]=human_preference_reward\n",
    "    env.close()\n",
    "    return action_probs_list, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822cc56d-b0a9-4b1d-b597-f78278ec1b81",
   "metadata": {},
   "source": [
    "This code defines a function **step_episode** that runs a single episode of an environment with a given agent (**model**).\n",
    "\n",
    "The function takes two inputs:\n",
    "\n",
    "- **env**: the environment the agent interacts with\n",
    "- **model**: the agent (**Actor** model)\n",
    "\n",
    "The function returns two outputs:\n",
    "\n",
    "- **action_probs_list**: a list of probability distribution of actions taken by the agent at each time step of the episode\n",
    "- **rewards**: a list of rewards collected by the agent at each time step of the episode\n",
    "\n",
    "The episode starts by resetting the environment and initializing some variables (**obs**, **action_probs_list**, **rewards**, and **done**). The function then enters a while loop and continues until the **done** flag is set (indicating that the episode has terminated).\n",
    "\n",
    "At each time step, the function:\n",
    "\n",
    "- Expands the current observation **obs** into a batch of size 1 (using **tf.expand_dims(obs, 0)**)\n",
    "- Runs the model (**Actor**) on the observation to get the action logits (**action_logits**)\n",
    "- Samples a categorical action index **selection_action_idx** using the logits as input to the Categorical distribution\n",
    "- Steps the environment using the selected action index (**env.step(selection_action_idx.numpy()[0])**) to get the new **observation**, **reward**, and **done** flag\n",
    "- Calculates the action probability using the Categorical distribution **action_probs = discrete_distribution.prob(selection_action_idx)**\n",
    "- Appends the action probability and reward to their respective lists (**action_probs_list** and **rewards**)\n",
    "- Finally, the function closes the environment and returns the **action_probs_list** and **rewards** lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "301b61b6-1148-463e-9dff-0480764be6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_g(reward_trajectory, gamma):\n",
    "    ez_discount = np.array([gamma**n for n in range(len(reward_trajectory))])\n",
    "    gs = []\n",
    "    reward_trajectory = np.array(reward_trajectory)\n",
    "    \n",
    "    for ts in range(len(reward_trajectory)):\n",
    "        to_end_reward = reward_trajectory[ts:]\n",
    "        eq_len_discount = ez_discount[:len(to_end_reward)]\n",
    "        total_value = np.multiply(to_end_reward, eq_len_discount)\n",
    "        g = sum(total_value)\n",
    "        gs.append(g)\n",
    "        \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd557de-47b2-4fa7-8feb-df0789827f2f",
   "metadata": {},
   "source": [
    "This function calculates the discounted cumulative rewards (i.e. the returns) for a given reward trajectory in reinforcement learning. The function takes two inputs:\n",
    "\n",
    "- **reward_trajectory**: a list of rewards collected in the trajectory of the agent.\n",
    "- **gamma**: the discount factor used in the calculation of the return.\n",
    "\n",
    "The function returns a list **gs** of the discounted cumulative rewards for each time step in the trajectory, with **gs[t]** being the return following time step **t**.\n",
    "\n",
    "The calculation is done by first creating an array of discounts **ez_discount** using the formula gamma**n for n in the range **0 to len(reward_trajectory)**. Then for each time step **ts** in the **reward_trajectory**, it calculates the rewards from that time step to the end of the trajectory, and discounts each reward by the appropriate discount in **eq_len_discount**. Finally, it multiplies each reward with its corresponding discount, and sums up the result to get the return (**g**). The return is then added to the list **gs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "397132e9-1284-46a6-a0a1-b75ffce4b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_loss_fn(action_probs, gs):\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = - tf.math.reduce_sum(action_log_probs * (gs))\n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d65dc5-527b-4d4c-bc49-4f9c41d4f7ff",
   "metadata": {},
   "source": [
    "This code defines a function **actor_loss_fn** that calculates the actor loss in reinforcement learning.\n",
    "\n",
    "The function takes two inputs:\n",
    "\n",
    "- **action_probs**: a list of probability distributions of actions taken by the agent at each time step of the episode\n",
    "- **gs**: a list of discounted returns at each time step of the episode\n",
    "\n",
    "The function returns the actor loss value (**actor_loss**).\n",
    "\n",
    "The function first calculates the log probability of the actions taken by the agent (**action_log_probs**) by taking the logarithm of **action_probs**.\n",
    "\n",
    "Then, it calculates the actor loss by performing an element-wise multiplication between **action_log_probs** and **gs** and reducing the sum of the result. The final loss value is negative to maximize the expected reward, as is common in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5222c52c-af89-41c5-994d-c70188d0c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_loss(action_probs, rewards):\n",
    "    gs = calc_g(rewards, .99)\n",
    "    actor_loss_val = actor_loss_fn(action_probs, gs)\n",
    "    return actor_loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46152b1-96b1-4413-b30a-9da7195e1371",
   "metadata": {},
   "source": [
    "This code defines a function **complete_loss** that calculates the total loss for an agent in a given episode.\n",
    "\n",
    "The function takes two inputs:\n",
    "\n",
    "- **action_probs**: a list of probability distribution of actions taken by the agent at each time step of the episode\n",
    "- **rewards**: a list of rewards collected by the agent at each time step of the episode\n",
    "\n",
    "The function returns the final loss value (**actor_loss_val**).\n",
    "\n",
    "The function first calls the **calc_g** function to calculate the discounted return (**gs**) based on the rewards and the discount factor (**0.99**).\n",
    "\n",
    "Then, it calculates the loss value (**actor_loss_val**) by calling the **actor_loss_fn** function and passing in the **action_probs** and **gs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe663f8f-353b-4c59-b46e-6a6d3886ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f748036f-c856-4db3-9756-f377ab3934c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you prefer action 0 leading to [ 0.06130395  0.4391379  -0.19123808 -1.3347244 ] over a random action? (1 for Yes, 0 for No):  0\n",
      "Do you prefer action 1 leading to [ 0.07612757  0.2526062  -0.25788748 -1.2478482 ] over action 0 leading to [ 0.06130395  0.4391379  -0.19123808 -1.3347244 ]? (1 for Yes, 0 for No):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you prefer action 0 leading to [-0.05062019 -1.4145433   0.14278352  2.165535  ] over a random action? (1 for Yes, 0 for No):  1\n",
      "Do you prefer action 1 leading to [-0.14726302 -1.6139232   0.29291096  2.6295507 ] over action 0 leading to [-0.05062019 -1.4145433   0.14278352  2.165535  ]? (1 for Yes, 0 for No):  0\n",
      "Do you prefer action 1 leading to [ 0.00186072  0.19027106 -0.1987237  -0.9627763 ] over a random action? (1 for Yes, 0 for No):  1\n",
      "Do you prefer action 0 leading to [ 0.02510552  0.39295053 -0.27745357 -1.4552069 ] over action 1 leading to [ 0.00186072  0.19027106 -0.1987237  -0.9627763 ]? (1 for Yes, 0 for No):  0\n",
      "Do you prefer action 1 leading to [-0.11770803 -1.026756    0.20149608  1.7081758 ] over a random action? (1 for Yes, 0 for No):  0\n",
      "Do you prefer action 0 leading to [-0.1677887  -0.84017986  0.29081836  1.6363155 ] over action 1 leading to [-0.11770803 -1.026756    0.20149608  1.7081758 ]? (1 for Yes, 0 for No):  0\n",
      "Do you prefer action 0 leading to [-0.15248846 -1.3333156   0.24607694  2.246089  ] over a random action? (1 for Yes, 0 for No):  1\n",
      "Do you prefer action 1 leading to [-0.24426545 -1.5338758   0.40245885  2.7973304 ] over action 0 leading to [-0.15248846 -1.3333156   0.24607694  2.246089  ]? (1 for Yes, 0 for No):  0\n",
      "Do you prefer action 1 leading to [-0.08407712 -1.0344257   0.18838215  1.703849  ] over a random action? (1 for Yes, 0 for No):  1\n",
      "Do you prefer action 0 leading to [-0.13460544 -0.847386    0.27715302  1.6193621 ] over action 1 leading to [-0.08407712 -1.0344257   0.18838215  1.703849  ]? (1 for Yes, 0 for No):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6.833333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2000\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m----> 4\u001b[0m         action_probs, episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mstep_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         loss \u001b[38;5;241m=\u001b[39m complete_loss(action_probs, episode_rewards)\n\u001b[1;32m      6\u001b[0m         grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[107], line 29\u001b[0m, in \u001b[0;36mstep_episode\u001b[0;34m(env, model)\u001b[0m\n\u001b[1;32m     27\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     28\u001b[0m     obs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(new_obs, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m human_preference_reward \u001b[38;5;241m=\u001b[39m \u001b[43mask_human_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39mhuman_preference_reward\n\u001b[1;32m     31\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[106], line 6\u001b[0m, in \u001b[0;36mask_human_preference\u001b[0;34m(state, action, next_state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_human_preference\u001b[39m(state, action, next_state):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Asks the human for their preference between the current action and a random alternative action.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Returns the reward based on the human's preference.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     current_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you prefer action \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maction\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m leading to \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnext_state\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m over a random action? (1 for Yes, 0 for No): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     random_action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m!=\u001b[39m action])\n\u001b[1;32m      8\u001b[0m     random_next_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(random_action)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1006\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[0;32m-> 1006\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1051\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "average_reward = []\n",
    "for episode in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs, episode_rewards = step_episode(env, model)\n",
    "        loss = complete_loss(action_probs, episode_rewards)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        average_reward.append(sum(episode_rewards)) # adds episode average reward\n",
    "        \n",
    "        if episode % 5 == 0:\n",
    "            print(episode, np.mean(average_reward[-25:]))\n",
    "    if np.mean(average_reward[-20:]) > 175:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d36e0-4e46-4497-8ce7-ad09fc43634c",
   "metadata": {},
   "source": [
    "This code implements the training loop for an actor-critic reinforcement learning agent. The training occurs over 2000 episodes or until the average reward over the last 20 episodes is greater than 175.\n",
    "\n",
    "For each episode:\n",
    "\n",
    "- The **step_episode** function is called to generate a set of action probabilities and episode rewards.\n",
    "- The **complete_loss** function is called to calculate the total loss for the episode, including both the actor loss and the critic loss.\n",
    "- The gradient of the loss with respect to the model's trainable variables is computed with **tape.gradient**.\n",
    "- The optimizer is then used to update the model's parameters.\n",
    "- The episode average reward is added to a list **average_reward**.\n",
    "\n",
    "If the episode number is divisible by 20, the mean of the last 25 average rewards is printed. If the mean average reward over the last 20 episodes is greater than 175, the training loop terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb0c97-c118-4888-ab95-9e3d1e242acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa132edb-7d66-438f-9bc4-873c8f9e1ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f64e21e1-5164-48e7-b377-5b729a0b4c59",
   "metadata": {},
   "source": [
    "# 2. Without Human Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0391f102-8a85-4ff8-90d3-7c3bf0ca1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_episode_originel(env, model):\n",
    "    obs = env.reset()[0]\n",
    "    action_probs_list = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    \n",
    "    while True:\n",
    "        if done:\n",
    "            break\n",
    "        obs = tf.expand_dims(obs, 0)\n",
    "        # run model to get action logits and value\n",
    "        action_logits = model(obs)\n",
    "        # categorical probabilistic action idx selection\n",
    "        discrete_distribution = tfp.distributions.Categorical(action_logits)\n",
    "        selection_action_idx = discrete_distribution.sample()\n",
    "        \n",
    "        action = selection_action_idx.numpy()[0]\n",
    "        \n",
    "        obs = env.step(action)[0]\n",
    "        reward = env.step(action)[1]\n",
    "        done = env.step(action)[2]\n",
    "        \n",
    "        action_probs = discrete_distribution.prob(selection_action_idx)\n",
    "        action_probs_list.append(action_probs)\n",
    "        rewards.append(reward)\n",
    "    env.close()\n",
    "    return action_probs_list, rewards, obs, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2e1ab075-9fdb-47cb-9e85-c0991c778cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.0\n",
      "50 5.2\n",
      "100 6.16\n",
      "150 5.72\n",
      "200 5.0\n",
      "250 8.12\n",
      "300 10.72\n",
      "350 14.4\n",
      "400 8.92\n",
      "450 10.88\n",
      "500 14.84\n",
      "550 11.28\n",
      "600 20.48\n",
      "650 22.24\n",
      "700 25.88\n",
      "750 37.2\n",
      "800 31.64\n",
      "850 35.68\n",
      "900 32.12\n",
      "950 40.04\n",
      "1000 45.68\n",
      "1050 55.4\n",
      "1100 61.76\n",
      "1150 53.52\n",
      "1200 40.6\n",
      "1250 72.24\n",
      "1300 60.52\n",
      "1350 83.76\n",
      "1400 85.32\n",
      "1450 73.2\n",
      "1500 90.24\n",
      "1550 92.08\n",
      "1600 86.32\n",
      "1650 122.0\n"
     ]
    }
   ],
   "source": [
    "average_reward = []\n",
    "for episode in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs, episode_rewards, obs, action = step_episode_originel(env, model)\n",
    "        loss = complete_loss(action_probs, episode_rewards)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        average_reward.append(sum(episode_rewards)) # adds episode average reward\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(episode, np.mean(average_reward[-25:]))\n",
    "    if np.mean(average_reward[-20:]) > 175:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7ea69ee3-a985-4fc6-8495-bca6dd753457",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(average_reward)\n",
    "\n",
    "# create a rolling window of size 20 and calculate the mean\n",
    "rolling_mean = s.rolling(window=20, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e2f78e8a-2923-4cac-8e13-fd481cd3a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rolling_mean)\n",
    "plt.title('Test of the model over 2000 episodes')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.savefig(\"final_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a93b0-aec5-42b4-85fd-5b73592c8601",
   "metadata": {},
   "source": [
    "# 3. With Human Preferences and fitting the Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b8c72c0-e7ce-4e56-82e3-7694f02bf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewarder(tf.keras.Model): # Reward function\n",
    "    def __init__(self, num_hidden_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.reward = layers.Dense(1)\n",
    "        \n",
    "    def call(self, input_obs):\n",
    "        x = self.shared_1(input_obs)\n",
    "        return self.reward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e776c72-039e-466d-9560-6e8afe129800",
   "metadata": {},
   "source": [
    "This code defines a class called **Rewarder** that inherits from **tf.keras.Model**, which means it is a TensorFlow Keras model. The purpose of this model is to calculate a reward value based on the input observations provided to it.\n",
    "\n",
    "The **__init__** method of the **Rewarder** class takes one parameter **num_hidden_units**, which specifies the number of hidden units to be used in the first layer of the model. Inside this method, two layers are defined: **self.shared_1** and **self.reward**.\n",
    "\n",
    "The first layer **self.shared_1** is an instance of the **Dense** class from the **layers** module of TensorFlow Keras. This layer has **num_hidden_units** hidden units and uses the rectified linear unit (ReLU) activation function.\n",
    "\n",
    "The second layer **self.reward** is another instance of the **Dense** class, but with only one output unit, which represents the predicted reward value.\n",
    "\n",
    "The **call** method of the **Rewarder** class is the method that is called when the model is run with input data. This method takes an input tensor **input_obs** as its argument and passes it through the first layer **self.shared_1** to produce an intermediate tensor **x**. The intermediate tensor **x** is then passed through the second layer **self.reward** to produce the final output tensor, which is the predicted reward value.\n",
    "\n",
    "Overall, this code defines a simple neural network model that takes an input tensor and produces a single output value that represents the predicted reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "814d1f72-dbfc-4986-9955-f2945036d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewarder = Rewarder(num_hidden_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a40178f-699a-49e5-b29f-4591d9c8b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "def step_episode_2(env, model):\n",
    "    env.reset()\n",
    "    action_probs_list = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    \n",
    "    while True:\n",
    "        if done:\n",
    "            break\n",
    "        obs = tf.expand_dims(env.state, 0)\n",
    "        # run model to get action logits and value\n",
    "        action_logits = model(obs)\n",
    "        # categorical probabilistic action idx selection\n",
    "        #discrete_distribution = tfp.distributions.Categorical(action_logits)\n",
    "        #selection_action_idx = discrete_distribution.sample()\n",
    "        #selected_action_idx = tf.random.categorical(action_logits, 1)[0, 0]\n",
    "        # categorical probabilistic action idx selection\n",
    "        discrete_distribution = tfp.distributions.Categorical(action_logits)\n",
    "        selection_action_idx = discrete_distribution.sample()\n",
    "        \n",
    "        states.append(obs)\n",
    "        actions.append(int(selection_action_idx.numpy()[0]))\n",
    "        \n",
    "        #reward, playing = env.step(selected_action_idx)\n",
    "        \n",
    "        action = int(selection_action_idx.numpy()[0])\n",
    "        obs = env.step(action)[0]\n",
    "        reward = env.step(action)[1]\n",
    "        done = env.step(action)[2]\n",
    "        \n",
    "\n",
    "        action_probs = discrete_distribution.prob(selection_action_idx)\n",
    "        action_probs_list.append(action_probs)\n",
    "        #action_probs = discrete_distribution.prob(selection_action_idx)\n",
    "        #action_probs_list.append(probability_of_taking_selected_action)\n",
    "        rewards.append(reward)\n",
    "    return action_probs_list, rewards, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9570ecf6-b377-4663-bc11-9f0e426e63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_update(states, actions, rewarder):\n",
    "    transition_ids = random.sample(range(0, len(states)-2), 2)\n",
    "    pref = input('selected preference a:left, d:right, s:same')\n",
    "    if pref == 'a':\n",
    "        dist = [1,0]\n",
    "    if pref == 'd':\n",
    "        dist = [0,1]\n",
    "    if pref == 's':\n",
    "        dist = [1,1]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        reward1 = rewarder(states[transition_ids[0]+1])\n",
    "        reward2 = rewarder(states[transition_ids[1]+1])\n",
    "        p1 = tf.exp(reward1) / tf.exp(reward1) + tf.exp(reward2)\n",
    "        p2 = tf.exp(reward2) / tf.exp(reward1) + tf.exp(reward2)\n",
    "        loss = -tf.math.log(p1)*dist[0] + tf.math.log(p2)*dist[1]\n",
    "    grads = tape.gradient(loss, rewarder.trainable_variables)\n",
    "    #print('\\n',len(grads),grads[0], grads[1].shape, grads[2].shape, grads[3].shape)\n",
    "    #print('\\n',len(rewarder.trainable_variables),rewarder.trainable_variables[0],rewarder.trainable_variables[1].shape,rewarder.trainable_variables[2].shape,rewarder.trainable_variables[3].shape)\n",
    "    optimizer.apply_gradients(zip(grads, rewarder.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d57016-f96e-4c78-8c43-91ecceda6952",
   "metadata": {},
   "source": [
    "The **preference_update** function takes three arguments: **states**, **actions**, and **rewarder**. This function updates the reward function **rewarder** based on user preferences between two randomly sampled state-action transitions.\n",
    "\n",
    "The function first randomly selects two state-action transitions from the provided **states** and **actions** arrays using the **random.sample** function. It then prompts the user to select a preference between the two transitions, with 'a' representing a preference for the left transition, 'd' representing a preference for the right transition, and 's' representing no preference.\n",
    "\n",
    "Based on the user's preference, the function creates a **dist** list with two elements representing the desired distribution of the reward probabilities for the two transitions.\n",
    "\n",
    "The function then uses TensorFlow's **GradientTape** context to compute the gradients of the loss with respect to the trainable variables in the **rewarder** model. The loss is defined based on the user's preference and the predicted reward values of the two state-action transitions. The loss is then used to compute the gradients, which are used to update the **rewarder** model using the provided **optimizer**.\n",
    "\n",
    "Overall, this code is used to update a reward function based on user preferences, with the goal of improving the performance of a reinforcement learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08fc55e8-3909-4319-bf0b-060dedb5c656",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "selected preference a:left, d:right, s:same a\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'The optimizer cannot recognize variable rewarder_1/dense_6/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m      8\u001b[0m average_len\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(rewards)) \u001b[38;5;66;03m# adds episode average reward\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mpreference_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewarder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m, episode)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage steps to target\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(average_len[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]))\n",
      "Cell \u001b[0;32mIn[97], line 20\u001b[0m, in \u001b[0;36mpreference_update\u001b[0;34m(states, actions, rewarder)\u001b[0m\n\u001b[1;32m     17\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, rewarder\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print('\\n',len(grads),grads[0], grads[1].shape, grads[2].shape, grads[3].shape)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print('\\n',len(rewarder.trainable_variables),rewarder.trainable_variables[0],rewarder.trainable_variables[1].shape,rewarder.trainable_variables[2].shape,rewarder.trainable_variables[3].shape)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewarder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1139\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    633\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 634\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_apply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1216\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1221\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2635\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2639\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2640\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3708\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3709\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3713\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   3718\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1213\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:216\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate different parts of the model separately. Please call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimizer.build(variables)` with the full list of trainable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables before the training loop or use legacy optimizer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{self.__class__.__name__}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(gradient, variable)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'The optimizer cannot recognize variable rewarder_1/dense_6/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'"
     ]
    }
   ],
   "source": [
    "average_len = []\n",
    "for episode in range(5000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs, rewards, states, actions = step_episode_2(env, model)\n",
    "        loss = complete_loss(action_probs, rewards)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    average_len.append(len(rewards)) # adds episode average reward\n",
    "\n",
    "    preference_update(states, actions, rewarder)\n",
    "    \n",
    "    print('Episode', episode)\n",
    "    \n",
    "    print('Average steps to target', np.mean(average_len[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062a754-577d-4a11-b317-724a7a40fbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
